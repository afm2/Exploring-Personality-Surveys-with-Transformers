{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personality Survey Item Generation for GitHub.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6YazhQVtWzU"
      },
      "source": [
        "BEFORE RUNNING: \n",
        "\n",
        "Set the Runtime Instance to use GPU Acceleration \n",
        "(Runtime -> Change Runtime Type -> Select \"GPU\" in the Hardware Accelerator dropdown menu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncfPbieBqdPz",
        "outputId": "c27518f8-bdef-482a-bed8-579ea899a772"
      },
      "source": [
        "from datetime import date, datetime\n",
        "current_date = date.today().strftime('%b-%d-%Y')\n",
        "current_time = datetime.now().time().strftime('%H:%M:%S')\n",
        "print(f'ITEM GENERATION START TIME: {current_date}, {current_time}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITEM GENERATION START TIME: Apr-24-2022, 17:12:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: In order to replicate the results from this study, the decoder_model should be set to 'GPT2-XL'. Presently, this model is too large to run with the resources alloted for free in Colab. A Colab Pro account provided sufficient resources at the time of writing to run this model. By default, the decoder_model is set to 'GPT2', as it can presently run successfully on a free GPU runtime instance. \n",
        "\n",
        "You may be able to run a larger model for free after lowering some of the optional arguments provided below. The values listed are those which were used in the final run used in the study."
      ],
      "metadata": {
        "id": "TKYF1gICopDv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRnY7J9NxO3Y"
      },
      "source": [
        "# RUN INFO\n",
        "#decoder_model = 'GPT2-XL' # Model used to generate item pool in study; REQUIRES HIGH RAM INSTANCE WITH COLAB PRO\n",
        "decoder_model = 'GPT2' # Model which can be run with a free colab GPU instance.\n",
        "sentence_encoder_model = 'Sentence_RoBERTa'\n",
        "# RUN SETTINGS\n",
        "stem_txt = 'I' # Common Stem\n",
        "fluff= -1 # Add this amount to a given facet's seq_limit, which is the average facet item length!\n",
        "n_item_pairs = 30 # How many random pairs of items do you want to create for every factor?\n",
        "\n",
        "# BEAM SEARCH ARGS\n",
        "#beam_search_n=1000 # n_beams\n",
        "n_gram_rule=2 # n_gram\n",
        "seqs_returned = 50 # return_seqs\n",
        "beam_k=30 # k for top_k sample in beam search\n",
        "beam_sample=.85 # p for top_p sample in beam search\n",
        "rep_penalty=5.0 # Repetition Penalty\n",
        "sample_temp=0.7 # Temperature of Sampling Distribution \n",
        "\n",
        "\n",
        "\n",
        " # GPT-2 Token ids for banned words \n",
        "banned_token_idx = []\n",
        "conjunctions_etc =  [[14508], [635], [392], [290], [18855], [606], [780], [13893], [31336], [2035], [270], [340], [273], [393], [14108], [534], [5832], [345]]\n",
        "banned_token_idx.extend(conjunctions_etc) #Banned Words: also, [space]also, and, [space]and, them, [space]them, because, [space]because, either, [space]either, your, [space]your, you, [space]you\n",
        "\n",
        "punctuation = [[1],[366], [198], [11], [553], [42911]]\n",
        "banned_token_idx.extend(punctuation) #\", [space]\", ,\", it, [space]it, or, [space]or, '\"', '[space]\"', '\\n',[space]',', ',\"', '[space],\"'\n",
        "\n",
        "actual_bad_words = [[33526], [562], [21551],[19317], [29836]]\n",
        "banned_token_idx.extend(actual_bad_words)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnzX74ekrkLQ"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvKnZ5MNsoD-"
      },
      "source": [
        "# Set Seeds...\n",
        "import numpy as np\n",
        "np.random.seed(2020)\n",
        "import random\n",
        "random.seed(2020)\n",
        "\n",
        "import torch\n",
        "torch.random.manual_seed(2020) \n",
        "\n",
        "import transformers\n",
        "transformers.set_seed(2020)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(2020)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload a copy of the 'HEXACO 100-item.csv' file included in this repository, or upload with another survey of your choosing and set the file name to df_path!"
      ],
      "metadata": {
        "id": "ZhKEi-GwrRzD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeExU4-r1JVB"
      },
      "source": [
        "df_path='./HEXACO 100-Items.csv'\n",
        "\n",
        "import regex\n",
        "tgt_survey = regex.sub(string = df_path, pattern = \"\\\\.\\/\", repl = \"\")\n",
        "tgt_survey = regex.sub(string = tgt_survey, pattern = \"\\\\.csv\", repl = \"\")\n",
        "export_path = f\"./{tgt_survey}|GPT-2 Large RoBERTa Large \" + current_date + \".xlsx\"\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TLb2zZ47GkL"
      },
      "source": [
        "### Load Pre-trained Transformers\n",
        "\n",
        "## Generative Model - GPT-2\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(decoder_model.lower()) # Need the decoder's tokenizer!\n",
        "tokenizer.eos_token = '\\.'\n",
        "tokenizer.pad_token = tokenizer.eos_token # gpt2 has no default pad token...\n",
        "\n",
        "# Model\n",
        "gpt2 =GPT2LMHeadModel.from_pretrained(decoder_model.lower()) # gpt2 no fine-tuning\n",
        "cuda=torch.device('cuda:0')\n",
        "gpt2.to(cuda)\n",
        "\n",
        "\n",
        "## Encoder Models - Sentence RoBERTa\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# STS Model\n",
        "sts_encoder = SentenceTransformer('stsb-roberta-large') # sentence roberta no fine-tuning\n",
        "# Paraphrase Model\n",
        "paraphraser = SentenceTransformer('paraphrase-distilroberta-base-v1') # distilled roberta no fine-tuning\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YQXkSej40MS"
      },
      "source": [
        "# Import all items from a csv file...\n",
        "import pandas as pd\n",
        "survey_items=pd.read_csv(f\"./{tgt_survey}.csv\")\n",
        "\n",
        "#Turn both columns into lists...\n",
        "item = survey_items.loc[:,'ITEM'].tolist() #item - the actual survey items\n",
        "info = survey_items.loc[:,'INFO'].tolist() #info - indicates beginning of new facet, indicates whether an item is + or - coded\n",
        "\n",
        "#Cleaning item info list...\n",
        "import regex\n",
        "import numpy as np\n",
        "\n",
        "#item_info = [regex.sub(string=i,  pattern=\" \\\\(.*\", repl=\"\") for i in info] # Remove reliability info from facet headers\n",
        "item_info = info\n",
        "\n",
        "for i in range(0,len(item_info)): # Replace blank lines with info for keying found in previous line.\n",
        "    if item_info[i] is \" \" or type(item_info[i]) is float:\n",
        "        item_info[i] = item_info[i-1]\n",
        "\n",
        "item_info = [regex.sub(string=item_i, pattern = \"\\\\xa0.*\",repl=\"\") for item_i in item_info] # Formatting requirement for McCrae Costa 30-facet scale\n",
        "\n",
        "item_dict={} \n",
        "for i in range(0,len(item_info)):\n",
        "    if not bool(regex.search(pattern=\"\\\\+ keyed\",string=item_info[i])) and not bool(regex.search(pattern='- keyed',string=item_info[i])):\n",
        "        curr_var = {}\n",
        "        j = i+1\n",
        "               \n",
        "        pos_key=[]\n",
        "        neg_key=[]\n",
        "        \n",
        "        while not bool(regex.search(pattern=\"^\\\\w\",string=item_info[j])):\n",
        "#        \n",
        "            if bool(regex.search(pattern=\"\\\\+ keyed\",string=item_info[j])):\n",
        "                #pos_key.append(\"I \" + item[j].lower())\n",
        "                pos_key.append(item[j])\n",
        "            if bool(regex.search(pattern=\"- keyed\",string=item_info[j])):\n",
        "                #neg_key.append(\"I \" + item[j].lower())\n",
        "                neg_key.append(item[j])\n",
        "#            \n",
        "            j+=1\n",
        "            if j == (len(item_info)-1):\n",
        "                break\n",
        "\n",
        "        curr_var['+'] = pos_key\n",
        "        curr_var['-'] = neg_key\n",
        "\n",
        "        item_dict[item_info[i]] = curr_var\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-fVs0zysg7"
      },
      "source": [
        "\n",
        "# Performs a beam search on a decoder transformer given some prompt text, and appends the output text of length k to a list. \n",
        "def get_sequence(model,prompt=str,word_limit=10,word_lower_limit=3,n_beams=1,n_gram=0,return_seqs=1,p=.5,prohibited_ids=banned_token_idx):\n",
        "    \n",
        "  #  assert n_beams >= return_seqs, 'n_beams must be greater than or equal to return_seqs!'\n",
        "\n",
        "    #beam_outputs=model.generate(**tokenizer([prompt],return_tensors='pt'),\n",
        "    beam_outputs=model.generate(**prompt,\n",
        "    max_length=word_limit,\n",
        "    min_length=word_lower_limit,\n",
        "    early_stopping=True,\n",
        "    #num_beams=n_beams,\n",
        "    no_repeat_ngram_size=n_gram,\n",
        "    num_return_sequences=return_seqs,\n",
        "    do_sample=True,\n",
        "    top_k=beam_k,\n",
        "    temperature=sample_temp, \n",
        "    repetition_penalty=rep_penalty,\n",
        "    top_p=p,\n",
        "    bad_words_ids=prohibited_ids,\n",
        "    early_stopping_rounds=True # stops generation of sequence at EOS token!\n",
        "    )\n",
        "    \n",
        "    seqs = []\n",
        "    for i, seq in enumerate(beam_outputs):\n",
        "        seqs.append(tokenizer.decode(seq.tolist()))\n",
        "    \n",
        "    return seqs\n",
        "\n",
        "# Simple function for cleaning and formatting sequence output. Drops all content before the first period (i.e. primer text) and after the second period (i.e., text for second sequence).\n",
        "def clean_sequence(seq, drop_primer=True, drop_fragments=True, drop_duplicates=True):\n",
        "    global lower_limit\n",
        "\n",
        "    for i in range(0,len(seq)):\n",
        "        if drop_primer:\n",
        "            seq[i] = regex.sub(string = seq[i], pattern = prompt_txt, repl=\"\")\n",
        "            seq[i] = \"I\" + seq[i]\n",
        "        #if drop_extra:        \n",
        "            \n",
        "            \n",
        "        # seq[i] = seq[i] + '.'\n",
        "        seq[i] = regex.sub(string = seq[i], pattern = \"\\n\\n.*\", repl=\"\")\n",
        "        #seq[i] = regex.sub(string = seq[i], pattern = \"\\n\", repl=\"\")\n",
        "        seq[i] = regex.sub(string = seq[i], pattern = \"\\..*$\", repl=\".\")\n",
        "        #seq[i] = regex.sub(string = seq[i], pattern = \",.*$\", repl=\"\")\n",
        "    #seq = [i for i in seq if len(regex.split(string=i,pattern=\" \")) > lower_limit]\n",
        "    if drop_fragments:\n",
        "      seq = [i for i in seq if bool(regex.search(string=i,pattern=\"\\.\"))] # Only keep sentences containing a period\n",
        "    \n",
        "    if drop_duplicates:\n",
        "      seq = list(set(seq)) # Remove duplicate items\n",
        "\n",
        "    return seq\n",
        "#%%\n",
        "\n",
        "def convergent(seqs, curr_key, item_list, sentence_transformer):\n",
        "    emb_items = torch.tensor(sentence_transformer.encode(item_list))\n",
        "    #emb_items_pos = sentence_transformer.encode(item_dict[curr_key]['+'])\n",
        "    #emb_items_neg = sentence_transformer.encode(item_dict[curr_key]['-'])\n",
        "    #emb_items = (torch.abs(torch.tensor(emb_items_pos)) + torch.abs(torch.tensor(emb_items_neg)))/2\n",
        "\n",
        "\n",
        "    emb_seqs = sentence_transformer.encode(seqs)\n",
        "    cos_sim = util.pytorch_cos_sim(emb_seqs, emb_items)\n",
        "    \n",
        "    tgt_cosines = torch.median(cos_sim,1).values\n",
        "    \n",
        "    \n",
        "    return tgt_cosines\n",
        "\n",
        "\n",
        "def discriminant(seqs, curr_key, item_list, sentence_transformer):\n",
        "\n",
        "    emb_items = torch.tensor(sentence_transformer.encode(item_list))\n",
        "\n",
        "    emb_seqs = sentence_transformer.encode(seqs)\n",
        "    cos_sim = util.pytorch_cos_sim(emb_seqs, emb_items)\n",
        "    \n",
        "    tgt_cosines = torch.median(cos_sim,1).values\n",
        "    \n",
        "\n",
        "    cosine_diff={}\n",
        "    for idx, key in enumerate(item_dict.keys()):\n",
        "        if key != curr_key:\n",
        "            curr_items = item_dict[key]['+'].copy()\n",
        "            curr_items.extend(item_dict[key]['-'])\n",
        "            emb_items = sentence_transformer.encode(curr_items)\n",
        "            cos_sim = util.pytorch_cos_sim(emb_seqs, emb_items)\n",
        "            median_cos = torch.median(cos_sim,1).values\n",
        "            \n",
        "            cosine_diff[key] = median_cos\n",
        "    \n",
        "    tensor_list = [cosine_diff[key] for key in cosine_diff.keys() ]\n",
        "    \n",
        "    n_factors = idx\n",
        "    non_tgt_mean_cosines = torch.abs(torch.cat(tensor_list).view(n_factors, tensor_list[0].shape[0]).transpose(-1,0)).median(dim=1).values \n",
        "\n",
        "    cosine_discriminant = tgt_cosines - non_tgt_mean_cosines\n",
        "    \n",
        "    return cosine_discriminant\n",
        "\n",
        "def internal_consistency(item_list, sentence_transformer):\n",
        "    import numpy as np\n",
        "    emb_items = sentence_transformer.encode(item_list)\n",
        "    cos_sim = util.pytorch_cos_sim(emb_items, emb_items)\n",
        "    \n",
        "    median_cosines = torch.median(cos_sim,1).values\n",
        "    \n",
        "    return np.round(median_cosines,4)\n",
        "\n",
        "#%%\n",
        "# Takes a list of generated text sequences, and calculates the average cosine similarity of each element with a list of survey items. \n",
        "# Returns the top k items with the highest average cosine similarity.\n",
        "\n",
        "def screen_item_pool(item_pool, cosines, sentence_transformer, curr_key,k=10):\n",
        "    import torch\n",
        "\n",
        "    assert type(cosines) == list, \"Cosines must be a list of lists!!!\"\n",
        "      \n",
        "      \n",
        "\n",
        "    \n",
        "    final_items = []\n",
        "    final_cosines = []\n",
        "\n",
        "    for cos in cosine_distinct:\n",
        "      # Get indices and cosines sorted by cosine similarity\n",
        "      idxs=torch.argsort(cos,0) # indices for matching with item pool idx\n",
        "      order=idxs.tolist() # indices as list for sorting idx and cosine value tensors\n",
        "      top_cosine = cos[order] # cosine values\n",
        "\n",
        "      #lb= torch.quantile(torch.absolute(torch.tensor(cos)),q=.9)\n",
        "      lb= torch.quantile(torch.absolute(torch.tensor(cos)),q=0)\n",
        "      ub= torch.quantile(torch.absolute(torch.tensor(cos)),q=1)\n",
        "\n",
        "      top_k = idxs[torch.where((torch.absolute(torch.tensor(cos)) >= lb) & (torch.absolute(torch.tensor(cos)) <= ub))].tolist()\n",
        "      top_cosine = cos[torch.where((torch.absolute(torch.tensor(cos)) >= lb) & (torch.absolute(torch.tensor(cos)) <= ub))].tolist() # cosine values\n",
        "\n",
        "### Take the list of indices to draw items from the pool\n",
        "      screened_items = [item_pool[int(j)] for j in top_k]\n",
        "      final_items.extend(screened_items)\n",
        "\n",
        "### Round list of cosine values for legibility\n",
        "      top_cosine = np.round(top_cosine,4).tolist()\n",
        "      final_cosines.extend(top_cosine)\n",
        "### Test output?\n",
        "# [si + \" : \" + str(tc) for (si, tc) in zip(screened_items, top_cosine)] \n",
        "    return screened_items, top_cosine\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the same generator and sentence encoder is used across all uses in this script, you may specify different models if you'd like to use on different scales, or even for differently keyed items within a given scale."
      ],
      "metadata": {
        "id": "Tiowv484sLYP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJogmKa1yvT"
      },
      "source": [
        "for idx, key in enumerate(item_dict.keys()):\n",
        "    \n",
        "    if regex.search('Honesty',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n",
        "    if regex.search('^Emotionality',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n",
        "    if regex.search('^Extraversion',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n",
        "    if regex.search('^Agreeableness',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n",
        "    if regex.search('^Conscientiousness',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n",
        "    if regex.search('^Openness',key):\n",
        "        item_dict[key]['pos_decoder'] = gpt2\n",
        "        item_dict[key]['neg_decoder'] = gpt2\n",
        "        item_dict[key]['sent_encoder'] = sts_encoder\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item Generation Block. For each generation occasion, a pair of items are randomly drawn from a given scale (regardless of their key) with replacement. A batch of items are collected cleaned, and appended to a dataframe."
      ],
      "metadata": {
        "id": "9tdk5-Ucs6pG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juYRGsKxEUMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d18b71-e882-4d9e-da8b-b5170e589b38"
      },
      "source": [
        "full_df = pd.DataFrame()\n",
        "\n",
        "import random\n",
        "\n",
        "for idx, key in enumerate(item_dict.keys()): # Iterate over all Traits...\n",
        "    item_list = item_dict[key]['+'].copy()\n",
        "    item_list.extend(item_dict[key]['-'])\n",
        "\n",
        "    prompt_txts = []\n",
        "    #n_item_pairs = 5\n",
        "    for n in range(0,n_item_pairs):\n",
        "      \n",
        "      i = 0\n",
        "      j = 0\n",
        "      while i == j: # ENSURE NO ITEM PAIRS ARE DUPLICATES...\n",
        "        i = random.choices(range(0, len(item_list)), k=1)[0]\n",
        "        j = random.choices(range(0, len(item_list)), k=1)[0]\n",
        "        \n",
        "      item_1 = item_list[i]\n",
        "      item_2 = item_list[j]\n",
        "      items = item_1 + ' ' + item_2 + ' ' + stem_txt\n",
        "      prompt_txts.append(items)\n",
        "\n",
        "\n",
        "    item_pool = []\n",
        "\n",
        "    for prompt_txt in prompt_txts:\n",
        "        seq_lengths=[len(regex.split(string=item_list[i], pattern= \" \")) + 1 for i in range(0,len(item_list))]\n",
        "        lower_limit=min(seq_lengths) \n",
        "        upper_limit=max(seq_lengths) + fluff\n",
        "        # SEQ_LIMIT DEPENDS ON THE ITEMS USED IN PROMPT, PLUS A FLUFF CONSTANT!\n",
        "\n",
        "        prompt_len = len(tokenizer(prompt_txt)['input_ids'])\n",
        "        seq_lower_limit= lower_limit + prompt_len\n",
        "        seq_upper_limit = upper_limit + prompt_len\n",
        "        \n",
        "        tokenized_prompt = tokenizer([prompt_txt],return_tensors='pt')\n",
        "        for tokenizer_key in tokenized_prompt.keys():\n",
        "            tokenized_prompt[tokenizer_key] = tokenized_prompt[tokenizer_key].to(cuda)\n",
        "\n",
        "        while True:\n",
        "            try:                  \n",
        "                new_seqs = get_sequence(gpt2,tokenized_prompt,word_limit=seq_upper_limit,\n",
        "                word_lower_limit=seq_lower_limit, prohibited_ids = banned_token_idx, #n_beams=beam_search_n,\n",
        "                n_gram=n_gram_rule,return_seqs=seqs_returned,p=beam_sample)\n",
        "                break\n",
        "            except UnboundLocalError:\n",
        "                print(f'Increasing seq_limit for {key} pos_decoder.')\n",
        "            finally:     \n",
        "                seq_upper_limit+=1\n",
        "\n",
        "\n",
        "        cleaned_seqs = clean_sequence(new_seqs)\n",
        "        \n",
        "        sts_cosine = convergent(seqs = cleaned_seqs, curr_key = key, item_list=item_list, sentence_transformer=sts_encoder)\n",
        "        distinctiveness = discriminant(seqs=cleaned_seqs, curr_key=key, item_list=item_list, sentence_transformer=paraphraser)\n",
        "        \n",
        "\n",
        "        curr_df = pd.DataFrame({\"Factor\":key, \"Item\":cleaned_seqs, \"STS Cosine\":sts_cosine, \"Distinct Cosine\":distinctiveness, \"Prompt\":np.repeat(prompt_txt,  [len(cleaned_seqs)], axis=0)}).drop_duplicates()\n",
        "        full_df = pd.concat([full_df, curr_df], axis=0).drop_duplicates()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cells below calculate some descriptive statistics that represent the semantic textual similarity (STS) of certain items compared to others (see code for the functions \"convergent\", \"discriminant\", and \"internal_consistency\" above).\n",
        "\n",
        "Although these statistics were not used to filter out or select specific items, they do appear informative of how an item relates to its prompts, as well as the other items generated, and the apparent key of items generated. "
      ],
      "metadata": {
        "id": "WpPA-Nftmp7p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBwGI7-fNsGr"
      },
      "source": [
        "internal_consistency_df = pd.DataFrame()\n",
        "for key in item_dict.keys():\n",
        "\n",
        "    factor_items = full_df[full_df.Factor.eq(key)].iloc[:,1].to_list()\n",
        "    \n",
        "    curr_ic = internal_consistency(item_list=factor_items, sentence_transformer = paraphraser)\n",
        "    curr_df = pd.DataFrame({'Factor':key,'Item':factor_items, 'Internal Consistency': curr_ic.tolist()})\n",
        "    internal_consistency_df = pd.concat([internal_consistency_df, curr_df], axis=0)\n",
        "\n",
        "\n",
        "full_df = full_df.merge(internal_consistency_df, on=['Factor', 'Item'])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPSFgJBWWQWK"
      },
      "source": [
        "final_pos_df = pd.DataFrame()\n",
        "final_neg_df = pd.DataFrame()\n",
        "for key in item_dict.keys():\n",
        "  \n",
        "  curr_list = full_df[full_df['Factor']==key]['Item'].to_list()\n",
        "  \n",
        "  \n",
        "  clip_prop = .0001\n",
        "\n",
        "  curr_df = pd.DataFrame()\n",
        "  embed_seqs = sts_encoder.encode(curr_list)\n",
        "  embed_items = sts_encoder.encode(item_dict[key]['+'])\n",
        "  cos_sim = util.pytorch_cos_sim(embed_seqs, embed_items)\n",
        "  keep_pos_items = torch.where(cos_sim > torch.quantile(torch.median(cos_sim,1).values,clip_prop))[0].tolist()\n",
        "  pos_med_cos = torch.median(cos_sim,1).values.tolist()\n",
        "  \n",
        "  curr_df = pd.DataFrame({'Factor':key,'Item':[curr_list[int(item)] for item in keep_pos_items], 'Pos Cosine':[pos_med_cos[int(item)] for item in keep_pos_items]})\n",
        "  final_pos_df = pd.concat([final_pos_df, curr_df], axis=0)\n",
        "\n",
        "  curr_df = pd.DataFrame()\n",
        "  embed_items = sts_encoder.encode(item_dict[key]['-'])\n",
        "  cos_sim = util.pytorch_cos_sim(embed_seqs, embed_items)\n",
        "  keep_neg_items = torch.where(cos_sim > torch.quantile(torch.median(cos_sim,1).values,clip_prop))[0].tolist()\n",
        "  neg_med_cos = torch.median(cos_sim,1).values.tolist()\n",
        "  \n",
        "  curr_df = pd.DataFrame({'Factor':key,'Item':[curr_list[int(item)] for item in keep_neg_items], 'Neg Cosine':[neg_med_cos[int(item)] for item in keep_neg_items]})\n",
        "  final_neg_df = pd.concat([final_neg_df, curr_df], axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny9PIdwBSMnD"
      },
      "source": [
        "final_df = final_pos_df.merge(final_neg_df, on = ['Factor', 'Item'])\n",
        "final_df = final_df.merge(full_df, on = ['Factor', 'Item'])\n",
        "final_df['Cos Diff'] = final_df['Pos Cosine'] - final_df['Neg Cosine']\n",
        "final_df = final_df.drop_duplicates(subset='Item')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgIJZwaCjT0E"
      },
      "source": [
        "#final_df2 = final_df.copy()\n",
        "internal_df = pd.DataFrame()\n",
        "for key in item_dict.keys():\n",
        "  item_list = final_df[final_df['Factor'] == key]['Item'].to_list()\n",
        "  sts_cosine = convergent(seqs = item_list, curr_key = key, item_list=item_list, sentence_transformer=sts_encoder)\n",
        "  distinctiveness = discriminant(seqs=item_list, curr_key=key, item_list=item_list, sentence_transformer=paraphraser)\n",
        "\n",
        "  curr_df = pd.DataFrame({'Factor':key,'Item':item_list, 'Internal STS':sts_cosine.tolist(),'Internal Distinct':distinctiveness.tolist()})\n",
        "  internal_df = pd.concat([internal_df, curr_df], axis=0)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XhIIE5m1dVH"
      },
      "source": [
        "final_df = final_df.merge(right=internal_df, on=['Factor','Item'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export results"
      ],
      "metadata": {
        "id": "-QTHS5Nqojes"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nF5QoEOE960"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "final_df.to_excel(export_path)\n",
        "files.download(export_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn9HojB33zMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32ec93f9-5d32-4688-9ead-0bc51586ae94"
      },
      "source": [
        "final_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      Factor  \\\n",
              "0           Honesty_Humility   \n",
              "1           Honesty_Humility   \n",
              "2           Honesty_Humility   \n",
              "3           Honesty_Humility   \n",
              "4           Honesty_Humility   \n",
              "...                      ...   \n",
              "4697  Openness to Experience   \n",
              "4698  Openness to Experience   \n",
              "4699  Openness to Experience   \n",
              "4700  Openness to Experience   \n",
              "4701  Openness to Experience   \n",
              "\n",
              "                                                   Item  Pos Cosine  \\\n",
              "0     I have the same kind-heartedness in me as othe...    0.274561   \n",
              "1     I think that people have to accept my opinions...    0.170223   \n",
              "2     I can only imagine what my life will be like i...    0.120150   \n",
              "3     I have to be prepared for all the hardships th...    0.233918   \n",
              "4     I have always been lucky to be in the business...    0.258920   \n",
              "...                                                 ...         ...   \n",
              "4697     I can't imagine what the next day will entail.    0.189039   \n",
              "4698              I just want the best for my children.    0.300048   \n",
              "4699  I'd have the same problem with reading books a...    0.020410   \n",
              "4700   I don't think that's the case with these things.    0.008531   \n",
              "4701  I'd rather just watch cartoons than go into th...    0.242764   \n",
              "\n",
              "      Neg Cosine  STS Cosine  Distinct Cosine  \\\n",
              "0       0.248132    0.274445        -0.044524   \n",
              "1       0.175738    0.175738        -0.022455   \n",
              "2       0.211231    0.129368         0.040330   \n",
              "3       0.367388    0.343129         0.090257   \n",
              "4       0.355786    0.325142         0.151571   \n",
              "...          ...         ...              ...   \n",
              "4697    0.136531    0.185055         0.055843   \n",
              "4698    0.169354    0.233278        -0.011982   \n",
              "4699    0.292826    0.220799         0.053008   \n",
              "4700    0.206919    0.080847         0.012990   \n",
              "4701    0.336264    0.290771         0.155772   \n",
              "\n",
              "                                                 Prompt  Internal Consistency  \\\n",
              "0     I would get a lot of pleasure from owning expe...                0.2765   \n",
              "1     I would get a lot of pleasure from owning expe...                0.2139   \n",
              "2     I would get a lot of pleasure from owning expe...                0.2152   \n",
              "3     I would get a lot of pleasure from owning expe...                0.2147   \n",
              "4     I would get a lot of pleasure from owning expe...                0.2072   \n",
              "...                                                 ...                   ...   \n",
              "4697  I’ve never really enjoyed looking through an e...                0.1735   \n",
              "4698  I’ve never really enjoyed looking through an e...                0.2394   \n",
              "4699  I’ve never really enjoyed looking through an e...                0.2118   \n",
              "4700  I’ve never really enjoyed looking through an e...                0.2836   \n",
              "4701  I’ve never really enjoyed looking through an e...                0.2578   \n",
              "\n",
              "      Cos Diff  Internal STS  Internal Distinct  \n",
              "0     0.026429      0.257434          -0.009495  \n",
              "1    -0.005515      0.181646          -0.026862  \n",
              "2    -0.091081      0.198688           0.048458  \n",
              "3    -0.133471      0.229444           0.023206  \n",
              "4    -0.096866      0.224660           0.046566  \n",
              "...        ...           ...                ...  \n",
              "4697  0.052508      0.221368           0.081211  \n",
              "4698  0.130694      0.329442           0.060241  \n",
              "4699 -0.272416      0.139491           0.032895  \n",
              "4700 -0.198387      0.122779           0.080835  \n",
              "4701 -0.093499      0.212596           0.096121  \n",
              "\n",
              "[4702 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0371c9aa-33cf-4842-b053-425cbd86d080\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Factor</th>\n",
              "      <th>Item</th>\n",
              "      <th>Pos Cosine</th>\n",
              "      <th>Neg Cosine</th>\n",
              "      <th>STS Cosine</th>\n",
              "      <th>Distinct Cosine</th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Internal Consistency</th>\n",
              "      <th>Cos Diff</th>\n",
              "      <th>Internal STS</th>\n",
              "      <th>Internal Distinct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Honesty_Humility</td>\n",
              "      <td>I have the same kind-heartedness in me as othe...</td>\n",
              "      <td>0.274561</td>\n",
              "      <td>0.248132</td>\n",
              "      <td>0.274445</td>\n",
              "      <td>-0.044524</td>\n",
              "      <td>I would get a lot of pleasure from owning expe...</td>\n",
              "      <td>0.2765</td>\n",
              "      <td>0.026429</td>\n",
              "      <td>0.257434</td>\n",
              "      <td>-0.009495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Honesty_Humility</td>\n",
              "      <td>I think that people have to accept my opinions...</td>\n",
              "      <td>0.170223</td>\n",
              "      <td>0.175738</td>\n",
              "      <td>0.175738</td>\n",
              "      <td>-0.022455</td>\n",
              "      <td>I would get a lot of pleasure from owning expe...</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.181646</td>\n",
              "      <td>-0.026862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Honesty_Humility</td>\n",
              "      <td>I can only imagine what my life will be like i...</td>\n",
              "      <td>0.120150</td>\n",
              "      <td>0.211231</td>\n",
              "      <td>0.129368</td>\n",
              "      <td>0.040330</td>\n",
              "      <td>I would get a lot of pleasure from owning expe...</td>\n",
              "      <td>0.2152</td>\n",
              "      <td>-0.091081</td>\n",
              "      <td>0.198688</td>\n",
              "      <td>0.048458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Honesty_Humility</td>\n",
              "      <td>I have to be prepared for all the hardships th...</td>\n",
              "      <td>0.233918</td>\n",
              "      <td>0.367388</td>\n",
              "      <td>0.343129</td>\n",
              "      <td>0.090257</td>\n",
              "      <td>I would get a lot of pleasure from owning expe...</td>\n",
              "      <td>0.2147</td>\n",
              "      <td>-0.133471</td>\n",
              "      <td>0.229444</td>\n",
              "      <td>0.023206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Honesty_Humility</td>\n",
              "      <td>I have always been lucky to be in the business...</td>\n",
              "      <td>0.258920</td>\n",
              "      <td>0.355786</td>\n",
              "      <td>0.325142</td>\n",
              "      <td>0.151571</td>\n",
              "      <td>I would get a lot of pleasure from owning expe...</td>\n",
              "      <td>0.2072</td>\n",
              "      <td>-0.096866</td>\n",
              "      <td>0.224660</td>\n",
              "      <td>0.046566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4697</th>\n",
              "      <td>Openness to Experience</td>\n",
              "      <td>I can't imagine what the next day will entail.</td>\n",
              "      <td>0.189039</td>\n",
              "      <td>0.136531</td>\n",
              "      <td>0.185055</td>\n",
              "      <td>0.055843</td>\n",
              "      <td>I’ve never really enjoyed looking through an e...</td>\n",
              "      <td>0.1735</td>\n",
              "      <td>0.052508</td>\n",
              "      <td>0.221368</td>\n",
              "      <td>0.081211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4698</th>\n",
              "      <td>Openness to Experience</td>\n",
              "      <td>I just want the best for my children.</td>\n",
              "      <td>0.300048</td>\n",
              "      <td>0.169354</td>\n",
              "      <td>0.233278</td>\n",
              "      <td>-0.011982</td>\n",
              "      <td>I’ve never really enjoyed looking through an e...</td>\n",
              "      <td>0.2394</td>\n",
              "      <td>0.130694</td>\n",
              "      <td>0.329442</td>\n",
              "      <td>0.060241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4699</th>\n",
              "      <td>Openness to Experience</td>\n",
              "      <td>I'd have the same problem with reading books a...</td>\n",
              "      <td>0.020410</td>\n",
              "      <td>0.292826</td>\n",
              "      <td>0.220799</td>\n",
              "      <td>0.053008</td>\n",
              "      <td>I’ve never really enjoyed looking through an e...</td>\n",
              "      <td>0.2118</td>\n",
              "      <td>-0.272416</td>\n",
              "      <td>0.139491</td>\n",
              "      <td>0.032895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4700</th>\n",
              "      <td>Openness to Experience</td>\n",
              "      <td>I don't think that's the case with these things.</td>\n",
              "      <td>0.008531</td>\n",
              "      <td>0.206919</td>\n",
              "      <td>0.080847</td>\n",
              "      <td>0.012990</td>\n",
              "      <td>I’ve never really enjoyed looking through an e...</td>\n",
              "      <td>0.2836</td>\n",
              "      <td>-0.198387</td>\n",
              "      <td>0.122779</td>\n",
              "      <td>0.080835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4701</th>\n",
              "      <td>Openness to Experience</td>\n",
              "      <td>I'd rather just watch cartoons than go into th...</td>\n",
              "      <td>0.242764</td>\n",
              "      <td>0.336264</td>\n",
              "      <td>0.290771</td>\n",
              "      <td>0.155772</td>\n",
              "      <td>I’ve never really enjoyed looking through an e...</td>\n",
              "      <td>0.2578</td>\n",
              "      <td>-0.093499</td>\n",
              "      <td>0.212596</td>\n",
              "      <td>0.096121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4702 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0371c9aa-33cf-4842-b053-425cbd86d080')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0371c9aa-33cf-4842-b053-425cbd86d080 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0371c9aa-33cf-4842-b053-425cbd86d080');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}